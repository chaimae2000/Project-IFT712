{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JKduE7-Nksu5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hpXC06Czksu5"
   },
   "outputs": [],
   "source": [
    "leaf_train_df = pd.read_csv(\"../data/train.csv\",index_col='id')\n",
    "leaf_test_df = pd.read_csv(\"../data/test.csv\",index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "id": "asymkmuRksu5",
    "outputId": "ac1dcaa7-4985-4906-bb80-8f3803e5b9f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1575</th>\n",
       "      <td>Magnolia_Salicifolia</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.119140</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148440</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>Acer_Pictum</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>Alnus_Maximowiczii</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>Quercus_Rubra</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.056641</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.030273</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>Quercus_Afares</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>990 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    species   margin1   margin2   margin3   margin4   margin5  \\\n",
       "id                                                                              \n",
       "1               Acer_Opalus  0.007812  0.023438  0.023438  0.003906  0.011719   \n",
       "2     Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625  0.025391   \n",
       "3      Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812  0.003906   \n",
       "5           Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859  0.021484   \n",
       "6        Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766  0.013672   \n",
       "...                     ...       ...       ...       ...       ...       ...   \n",
       "1575   Magnolia_Salicifolia  0.060547  0.119140  0.007812  0.003906  0.000000   \n",
       "1578            Acer_Pictum  0.001953  0.003906  0.021484  0.107420  0.001953   \n",
       "1581     Alnus_Maximowiczii  0.001953  0.003906  0.000000  0.021484  0.078125   \n",
       "1582          Quercus_Rubra  0.000000  0.000000  0.046875  0.056641  0.009766   \n",
       "1584         Quercus_Afares  0.023438  0.019531  0.031250  0.015625  0.005859   \n",
       "\n",
       "       margin6   margin7  margin8   margin9  ...  texture55  texture56  \\\n",
       "id                                           ...                         \n",
       "1     0.009766  0.027344      0.0  0.001953  ...   0.007812   0.000000   \n",
       "2     0.001953  0.019531      0.0  0.000000  ...   0.000977   0.000000   \n",
       "3     0.005859  0.068359      0.0  0.000000  ...   0.154300   0.000000   \n",
       "5     0.019531  0.023438      0.0  0.013672  ...   0.000000   0.000977   \n",
       "6     0.015625  0.005859      0.0  0.000000  ...   0.096680   0.000000   \n",
       "...        ...       ...      ...       ...  ...        ...        ...   \n",
       "1575  0.148440  0.017578      0.0  0.001953  ...   0.242190   0.000000   \n",
       "1578  0.000000  0.000000      0.0  0.029297  ...   0.170900   0.000000   \n",
       "1581  0.003906  0.007812      0.0  0.003906  ...   0.004883   0.000977   \n",
       "1582  0.000000  0.000000      0.0  0.037109  ...   0.083008   0.030273   \n",
       "1584  0.019531  0.035156      0.0  0.003906  ...   0.000000   0.000000   \n",
       "\n",
       "      texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "id                                                                       \n",
       "1      0.002930   0.002930   0.035156   0.000000   0.000000   0.004883   \n",
       "2      0.000000   0.000977   0.023438   0.000000   0.000000   0.000977   \n",
       "3      0.005859   0.000977   0.007812   0.000000   0.000000   0.000000   \n",
       "5      0.000000   0.000000   0.020508   0.000000   0.000000   0.017578   \n",
       "6      0.021484   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1575   0.034180   0.000000   0.010742   0.000000   0.000000   0.000000   \n",
       "1578   0.018555   0.000000   0.011719   0.000000   0.000000   0.000977   \n",
       "1581   0.004883   0.027344   0.016602   0.007812   0.000000   0.027344   \n",
       "1582   0.000977   0.002930   0.014648   0.000000   0.041992   0.000000   \n",
       "1584   0.002930   0.000000   0.012695   0.000000   0.000000   0.023438   \n",
       "\n",
       "      texture63  texture64  \n",
       "id                          \n",
       "1      0.000000   0.025391  \n",
       "2      0.039062   0.022461  \n",
       "3      0.020508   0.002930  \n",
       "5      0.000000   0.047852  \n",
       "6      0.000000   0.031250  \n",
       "...         ...        ...  \n",
       "1575   0.000000   0.018555  \n",
       "1578   0.000000   0.021484  \n",
       "1581   0.000000   0.001953  \n",
       "1582   0.001953   0.002930  \n",
       "1584   0.025391   0.022461  \n",
       "\n",
       "[990 rows x 193 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaf_train_df['species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "id": "LZSA-m6Bksu6",
    "outputId": "e7d2a907-0d87-494e-874b-b76fef3db051"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>margin10</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1577</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090820</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1579</th>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.006836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.018555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136720</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107420</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
       "id                                                                           \n",
       "4     0.019531  0.009766  0.078125  0.011719  0.003906  0.015625  0.005859   \n",
       "7     0.007812  0.005859  0.064453  0.009766  0.003906  0.013672  0.007812   \n",
       "9     0.000000  0.000000  0.001953  0.021484  0.041016  0.000000  0.023438   \n",
       "12    0.000000  0.000000  0.009766  0.011719  0.017578  0.000000  0.003906   \n",
       "13    0.001953  0.000000  0.015625  0.009766  0.039062  0.000000  0.009766   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1576  0.000000  0.000000  0.003906  0.015625  0.041016  0.000000  0.017578   \n",
       "1577  0.000000  0.003906  0.003906  0.005859  0.017578  0.000000  0.017578   \n",
       "1579  0.017578  0.029297  0.015625  0.013672  0.003906  0.015625  0.025391   \n",
       "1580  0.013672  0.009766  0.060547  0.025391  0.035156  0.025391  0.039062   \n",
       "1583  0.000000  0.117190  0.000000  0.019531  0.000000  0.136720  0.001953   \n",
       "\n",
       "       margin8   margin9  margin10  ...  texture55  texture56  texture57  \\\n",
       "id                                  ...                                    \n",
       "4     0.000000  0.005859  0.023438  ...   0.006836   0.000000   0.015625   \n",
       "7     0.000000  0.033203  0.023438  ...   0.000000   0.000000   0.006836   \n",
       "9     0.000000  0.011719  0.005859  ...   0.128910   0.000000   0.000977   \n",
       "12    0.000000  0.003906  0.001953  ...   0.012695   0.015625   0.002930   \n",
       "13    0.000000  0.005859  0.000000  ...   0.000000   0.042969   0.016602   \n",
       "...        ...       ...       ...  ...        ...        ...        ...   \n",
       "1576  0.000000  0.005859  0.013672  ...   0.098633   0.000000   0.004883   \n",
       "1577  0.005859  0.000000  0.005859  ...   0.012695   0.004883   0.004883   \n",
       "1579  0.000000  0.000000  0.009766  ...   0.073242   0.000000   0.028320   \n",
       "1580  0.000000  0.003906  0.023438  ...   0.003906   0.000000   0.000977   \n",
       "1583  0.005859  0.000000  0.007812  ...   0.107420   0.012695   0.016602   \n",
       "\n",
       "      texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "id                                                                       \n",
       "4      0.000977   0.015625        0.0        0.0   0.000000   0.003906   \n",
       "7      0.001953   0.013672        0.0        0.0   0.000977   0.037109   \n",
       "9      0.000000   0.000000        0.0        0.0   0.015625   0.000000   \n",
       "12     0.036133   0.013672        0.0        0.0   0.089844   0.000000   \n",
       "13     0.010742   0.041016        0.0        0.0   0.007812   0.009766   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1576   0.000000   0.003906        0.0        0.0   0.018555   0.000000   \n",
       "1577   0.002930   0.009766        0.0        0.0   0.090820   0.000000   \n",
       "1579   0.000000   0.001953        0.0        0.0   0.000000   0.042969   \n",
       "1580   0.000000   0.011719        0.0        0.0   0.000000   0.011719   \n",
       "1583   0.000977   0.004883        0.0        0.0   0.015625   0.000000   \n",
       "\n",
       "      texture64  \n",
       "id               \n",
       "4      0.053711  \n",
       "7      0.044922  \n",
       "9      0.000000  \n",
       "12     0.008789  \n",
       "13     0.007812  \n",
       "...         ...  \n",
       "1576   0.000977  \n",
       "1577   0.016602  \n",
       "1579   0.006836  \n",
       "1580   0.018555  \n",
       "1583   0.017578  \n",
       "\n",
       "[594 rows x 192 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mH54epkwksu6"
   },
   "outputs": [],
   "source": [
    "train_data, train_target = leaf_train_df[leaf_train_df.columns.difference(['species'])], leaf_train_df['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ft/9dhchwd92nq_lyrh2_7rvqn00000gn/T/ipykernel_47378/1497557196.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m xtrain, xtest, ytrain, ytest = train_test_split(leaf_train_df.iloc[:,2:195], leaf_train_df['species'], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                 train_size=0.5, stratify = leaf_train_df['species'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(leaf_train_df.iloc[:,2:195], leaf_train_df['species'], \n",
    "                                                train_size=0.5, stratify = leaf_train_df['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2z0J2Gwksu6"
   },
   "source": [
    "## 192 dimensions, 99 classes, 990 données train, 594 données de test (1,584 en total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMKy54MHksu6"
   },
   "source": [
    "## SVM Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "iar7MKiRksu6"
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UV2pJIvAksu7"
   },
   "outputs": [],
   "source": [
    "C = [1, 10, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "Rv2o-A4Vksu7"
   },
   "outputs": [],
   "source": [
    "parameters_grid = [\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['poly'], 'degree':list(range(2,9)), 'coef0':list(np.linspace(0.000001, 2, num=10))},\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma':['scale','auto'], 'gamma':list(np.linspace(0.000001, 2, num=10))},\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['sigmoid'], 'coef0':list(np.linspace(0.000001, 2, num=10))}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "7oYKT1E9ksu7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
       "                         {'C': [1, 10, 100, 1000],\n",
       "                          'coef0': [1e-06, 0.22222311111111112,\n",
       "                                    0.4444452222222222, 0.6666673333333334,\n",
       "                                    0.8888894444444445, 1.1111115555555555,\n",
       "                                    1.3333336666666666, 1.5555557777777778,\n",
       "                                    1.7777778888888889, 2.0],\n",
       "                          'degree': [2, 3, 4, 5, 6, 7, 8], 'kernel': ['poly']},\n",
       "                         {'C': [1, 10, 100...\n",
       "                                    0.4444452222222222, 0.6666673333333334,\n",
       "                                    0.8888894444444445, 1.1111115555555555,\n",
       "                                    1.3333336666666666, 1.5555557777777778,\n",
       "                                    1.7777778888888889, 2.0],\n",
       "                          'kernel': ['rbf']},\n",
       "                         {'C': [1, 10, 100, 1000],\n",
       "                          'coef0': [1e-06, 0.22222311111111112,\n",
       "                                    0.4444452222222222, 0.6666673333333334,\n",
       "                                    0.8888894444444445, 1.1111115555555555,\n",
       "                                    1.3333336666666666, 1.5555557777777778,\n",
       "                                    1.7777778888888889, 2.0],\n",
       "                          'kernel': ['sigmoid']}])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters_grid, cv=5, n_jobs=-1)\n",
    "clf.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "SUvD_rJlksu7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 0.4444452222222222, 'kernel': 'rbf'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = clf.best_params_\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kZBeUgSEksu7"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "c0yfPRugksu7",
    "outputId": "fe0fb705-e7ba-4682-f3ef-12dc48f0cf03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_Macro Linear 0.9424915824915825  (+/-)  0.030710024076770412\n"
     ]
    }
   ],
   "source": [
    "clf_linear = svm.SVC(kernel= 'linear', C = 100)\n",
    "scores_linear = cross_val_score(clf_linear, train_data, train_target, cv=5, scoring='f1_macro')\n",
    "print(\"F1_Macro Linear\",scores_linear.mean(),\" (+/-) \", scores_linear.std() * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "gNJVY2rNksu7",
    "outputId": "bfb7b8c2-0f0b-4a74-9c7e-e3c1b6f08966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_Macro Rbf 0.9413468013468013  (+/-)  0.03683790975904359\n"
     ]
    }
   ],
   "source": [
    "clf_rbf = svm.SVC(kernel= 'rbf', gamma = 0.4444452222222222, C = 100)\n",
    "scores_rbf = cross_val_score(clf_rbf, train_data, train_target, cv=5, scoring='f1_macro')\n",
    "print(\"F1_Macro Rbf\",scores_rbf.mean(),\" (+/-) \", scores_rbf.std() * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9191919191919192\n",
      "-***-\n",
      "Confusion matrix:\n",
      "[[5 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 5 0 0]\n",
      " [0 0 0 ... 0 5 0]\n",
      " [0 0 0 ... 0 0 4]]\n",
      "-***-\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       0.83      1.00      0.91         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         5\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         5\n",
      "          11       0.83      1.00      0.91         5\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       1.00      1.00      1.00         5\n",
      "          14       1.00      1.00      1.00         5\n",
      "          15       0.83      1.00      0.91         5\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       0.83      1.00      0.91         5\n",
      "          18       1.00      1.00      1.00         5\n",
      "          19       0.83      1.00      0.91         5\n",
      "          20       1.00      1.00      1.00         5\n",
      "          21       1.00      1.00      1.00         5\n",
      "          22       0.83      1.00      0.91         5\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       1.00      1.00      1.00         5\n",
      "          25       1.00      1.00      1.00         5\n",
      "          26       1.00      0.60      0.75         5\n",
      "          27       0.75      0.60      0.67         5\n",
      "          28       0.75      0.60      0.67         5\n",
      "          29       0.67      0.80      0.73         5\n",
      "          30       1.00      0.80      0.89         5\n",
      "          31       1.00      0.60      0.75         5\n",
      "          32       0.83      1.00      0.91         5\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       0.67      0.40      0.50         5\n",
      "          35       0.83      1.00      0.91         5\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      0.60      0.75         5\n",
      "          38       0.71      1.00      0.83         5\n",
      "          39       1.00      1.00      1.00         5\n",
      "          40       0.62      1.00      0.77         5\n",
      "          41       1.00      1.00      1.00         5\n",
      "          42       0.83      1.00      0.91         5\n",
      "          43       0.83      1.00      0.91         5\n",
      "          44       1.00      0.60      0.75         5\n",
      "          45       1.00      1.00      1.00         5\n",
      "          46       1.00      0.80      0.89         5\n",
      "          47       1.00      0.80      0.89         5\n",
      "          48       1.00      1.00      1.00         5\n",
      "          49       1.00      1.00      1.00         5\n",
      "          50       0.60      0.60      0.60         5\n",
      "          51       0.83      1.00      0.91         5\n",
      "          52       0.71      1.00      0.83         5\n",
      "          53       0.71      1.00      0.83         5\n",
      "          54       1.00      1.00      1.00         5\n",
      "          55       0.57      0.80      0.67         5\n",
      "          56       0.83      1.00      0.91         5\n",
      "          57       1.00      1.00      1.00         5\n",
      "          58       0.83      1.00      0.91         5\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      1.00      1.00         5\n",
      "          61       1.00      1.00      1.00         5\n",
      "          62       1.00      1.00      1.00         5\n",
      "          63       1.00      0.80      0.89         5\n",
      "          64       1.00      0.40      0.57         5\n",
      "          65       1.00      1.00      1.00         5\n",
      "          66       0.71      1.00      0.83         5\n",
      "          67       1.00      1.00      1.00         5\n",
      "          68       0.83      1.00      0.91         5\n",
      "          69       1.00      0.80      0.89         5\n",
      "          70       1.00      1.00      1.00         5\n",
      "          71       1.00      0.80      0.89         5\n",
      "          72       1.00      1.00      1.00         5\n",
      "          73       1.00      0.80      0.89         5\n",
      "          74       1.00      1.00      1.00         5\n",
      "          75       1.00      1.00      1.00         5\n",
      "          76       1.00      1.00      1.00         5\n",
      "          77       1.00      0.80      0.89         5\n",
      "          78       1.00      1.00      1.00         5\n",
      "          79       0.80      0.80      0.80         5\n",
      "          80       0.71      1.00      0.83         5\n",
      "          81       1.00      0.60      0.75         5\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       0.80      0.80      0.80         5\n",
      "          84       1.00      1.00      1.00         5\n",
      "          85       0.83      1.00      0.91         5\n",
      "          86       1.00      0.60      0.75         5\n",
      "          87       1.00      1.00      1.00         5\n",
      "          88       1.00      1.00      1.00         5\n",
      "          89       1.00      1.00      1.00         5\n",
      "          90       1.00      1.00      1.00         5\n",
      "          91       1.00      1.00      1.00         5\n",
      "          92       1.00      1.00      1.00         5\n",
      "          93       1.00      1.00      1.00         5\n",
      "          94       1.00      1.00      1.00         5\n",
      "          95       1.00      0.80      0.89         5\n",
      "          96       1.00      1.00      1.00         5\n",
      "          97       1.00      1.00      1.00         5\n",
      "          98       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.92       495\n",
      "   macro avg       0.93      0.92      0.92       495\n",
      "weighted avg       0.93      0.92      0.92       495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred_svc = clf.predict(xtest)\n",
    "acc_score = accuracy_score(ytest, ypred_svc)\n",
    "conf_matrix = confusion_matrix(ytest, ypred_svc)\n",
    "class_report = classification_report(ytest, ypred_svc)\n",
    "print('Accuracy:', acc_score)\n",
    "print('-***-')\n",
    "print('Confusion matrix:')\n",
    "print(conf_matrix)\n",
    "print('-***-')\n",
    "print('Classification report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM One Against-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "leaf_train_df['species'] = labelencoder.fit_transform(leaf_train_df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur hyperparamètre -entraînement: {'C': 1000.0, 'multi_class': 'ovr', 'penalty': 'l1'} tel que accuracy est 0.923\n",
      "\n",
      " la validation croisée :\n",
      "\taccuracy = 0.790 (+/-0.163) for {'C': 0.001, 'multi_class': 'crammer_singer', 'penalty': 'l1'}\n",
      "\taccuracy = 0.784 (+/-0.112) for {'C': 0.001, 'multi_class': 'crammer_singer', 'penalty': 'l2'}\n",
      "\taccuracy = 0.010 (+/-0.000) for {'C': 0.001, 'multi_class': 'ovr', 'penalty': 'l1'}\n",
      "\taccuracy = 0.515 (+/-0.031) for {'C': 0.001, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\taccuracy = 0.842 (+/-0.061) for {'C': 0.01, 'multi_class': 'crammer_singer', 'penalty': 'l1'}\n",
      "\taccuracy = 0.844 (+/-0.062) for {'C': 0.01, 'multi_class': 'crammer_singer', 'penalty': 'l2'}\n",
      "\taccuracy = 0.010 (+/-0.000) for {'C': 0.01, 'multi_class': 'ovr', 'penalty': 'l1'}\n",
      "\taccuracy = 0.541 (+/-0.052) for {'C': 0.01, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\taccuracy = 0.840 (+/-0.058) for {'C': 0.1, 'multi_class': 'crammer_singer', 'penalty': 'l1'}\n",
      "\taccuracy = 0.840 (+/-0.058) for {'C': 0.1, 'multi_class': 'crammer_singer', 'penalty': 'l2'}\n",
      "\taccuracy = 0.010 (+/-0.000) for {'C': 0.1, 'multi_class': 'ovr', 'penalty': 'l1'}\n",
      "\taccuracy = 0.604 (+/-0.064) for {'C': 0.1, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\taccuracy = 0.840 (+/-0.058) for {'C': 1.0, 'multi_class': 'crammer_singer', 'penalty': 'l1'}\n",
      "\taccuracy = 0.840 (+/-0.058) for {'C': 1.0, 'multi_class': 'crammer_singer', 'penalty': 'l2'}\n",
      "\taccuracy = 0.222 (+/-0.049) for {'C': 1.0, 'multi_class': 'ovr', 'penalty': 'l1'}\n",
      "\taccuracy = 0.798 (+/-0.110) for {'C': 1.0, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\taccuracy = 0.846 (+/-0.074) for {'C': 10.0, 'multi_class': 'crammer_singer', 'penalty': 'l1'}\n",
      "\taccuracy = 0.846 (+/-0.074) for {'C': 10.0, 'multi_class': 'crammer_singer', 'penalty': 'l2'}\n",
      "\taccuracy = 0.828 (+/-0.068) for {'C': 10.0, 'multi_class': 'ovr', 'penalty': 'l1'}\n",
      "\taccuracy = 0.893 (+/-0.049) for {'C': 10.0, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\taccuracy = 0.911 (+/-0.062) for {'C': 100.0, 'multi_class': 'crammer_singer', 'penalty': 'l1'}\n",
      "\taccuracy = 0.911 (+/-0.062) for {'C': 100.0, 'multi_class': 'crammer_singer', 'penalty': 'l2'}\n",
      "\taccuracy = 0.911 (+/-0.062) for {'C': 100.0, 'multi_class': 'ovr', 'penalty': 'l1'}\n",
      "\taccuracy = 0.921 (+/-0.050) for {'C': 100.0, 'multi_class': 'ovr', 'penalty': 'l2'}\n",
      "\taccuracy = 0.913 (+/-0.067) for {'C': 1000.0, 'multi_class': 'crammer_singer', 'penalty': 'l1'}\n",
      "\taccuracy = 0.913 (+/-0.067) for {'C': 1000.0, 'multi_class': 'crammer_singer', 'penalty': 'l2'}\n",
      "\taccuracy = 0.923 (+/-0.071) for {'C': 1000.0, 'multi_class': 'ovr', 'penalty': 'l1'}\n",
      "\taccuracy = 0.911 (+/-0.050) for {'C': 1000.0, 'multi_class': 'ovr', 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "StratifiedKFold = StratifiedKFold(n_splits=5, random_state=11, shuffle=True)\n",
    "svm = LinearSVC(max_iter = 100, dual = False)\n",
    "params = {  'C': np.logspace(-3, 3, 7),'multi_class' : [\"crammer_singer\",\"ovr\"],'penalty' : [\"l1\",\"l2\"] }\n",
    "GridSearchCV_svm = GridSearchCV(svm, params,scoring = 'accuracy',cv = StratifiedKFold)\n",
    "GridSearchCV_svm.fit(xtrain, ytrain)\n",
    "print(\"Meilleur hyperparamètre -entraînement:\",GridSearchCV_svm.best_params_, \n",
    "      \"tel que accuracy est\", round(GridSearchCV_svm.cv_results_['mean_test_score'].max(), 3))\n",
    "print(\"\\n la validation croisée :\")\n",
    "for mean, std, params in zip(GridSearchCV_svm.cv_results_['mean_test_score'], # score moyen\n",
    "    GridSearchCV_svm.cv_results_['std_test_score'], # écart type  \n",
    "    GridSearchCV_svm.cv_results_['params'] ):\n",
    "    print(\"\\t%s = %0.3f (+/-%0.03f) for %r\" % ('accuracy',mean, std * 2,params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " pour le test l'accuracy est de 0.937\n"
     ]
    }
   ],
   "source": [
    "ypred = GridSearchCV_svm.predict(xtest)\n",
    "print(\"\\n pour le test l'accuracy est de %0.3f\" % metrics.accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9373737373737374\n"
     ]
    }
   ],
   "source": [
    "acc_score = accuracy_score(ytest, ypred)\n",
    "print('Accuracy:', acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[5 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 5 0 0]\n",
      " [0 0 0 ... 0 5 0]\n",
      " [0 0 0 ... 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(ytest, ypred)\n",
    "print('Confusion matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      0.80      0.89         5\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         5\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         5\n",
      "          11       0.83      1.00      0.91         5\n",
      "          12       0.75      0.60      0.67         5\n",
      "          13       0.71      1.00      0.83         5\n",
      "          14       1.00      1.00      1.00         5\n",
      "          15       1.00      1.00      1.00         5\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      0.80      0.89         5\n",
      "          18       1.00      0.60      0.75         5\n",
      "          19       1.00      1.00      1.00         5\n",
      "          20       1.00      1.00      1.00         5\n",
      "          21       1.00      0.80      0.89         5\n",
      "          22       1.00      0.80      0.89         5\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       1.00      1.00      1.00         5\n",
      "          25       1.00      1.00      1.00         5\n",
      "          26       1.00      1.00      1.00         5\n",
      "          27       0.71      1.00      0.83         5\n",
      "          28       1.00      0.80      0.89         5\n",
      "          29       1.00      1.00      1.00         5\n",
      "          30       1.00      0.80      0.89         5\n",
      "          31       1.00      0.80      0.89         5\n",
      "          32       0.83      1.00      0.91         5\n",
      "          33       1.00      0.80      0.89         5\n",
      "          34       1.00      1.00      1.00         5\n",
      "          35       0.71      1.00      0.83         5\n",
      "          36       1.00      0.40      0.57         5\n",
      "          37       0.83      1.00      0.91         5\n",
      "          38       1.00      0.80      0.89         5\n",
      "          39       0.71      1.00      0.83         5\n",
      "          40       1.00      0.80      0.89         5\n",
      "          41       0.67      0.80      0.73         5\n",
      "          42       1.00      1.00      1.00         5\n",
      "          43       0.83      1.00      0.91         5\n",
      "          44       1.00      0.80      0.89         5\n",
      "          45       0.62      1.00      0.77         5\n",
      "          46       1.00      0.80      0.89         5\n",
      "          47       1.00      0.80      0.89         5\n",
      "          48       1.00      0.80      0.89         5\n",
      "          49       1.00      1.00      1.00         5\n",
      "          50       1.00      1.00      1.00         5\n",
      "          51       1.00      1.00      1.00         5\n",
      "          52       0.67      0.80      0.73         5\n",
      "          53       1.00      1.00      1.00         5\n",
      "          54       1.00      1.00      1.00         5\n",
      "          55       1.00      1.00      1.00         5\n",
      "          56       1.00      1.00      1.00         5\n",
      "          57       0.83      1.00      0.91         5\n",
      "          58       1.00      1.00      1.00         5\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      1.00      1.00         5\n",
      "          61       1.00      1.00      1.00         5\n",
      "          62       1.00      1.00      1.00         5\n",
      "          63       0.71      1.00      0.83         5\n",
      "          64       1.00      1.00      1.00         5\n",
      "          65       1.00      1.00      1.00         5\n",
      "          66       0.83      1.00      0.91         5\n",
      "          67       0.83      1.00      0.91         5\n",
      "          68       0.83      1.00      0.91         5\n",
      "          69       1.00      1.00      1.00         5\n",
      "          70       1.00      1.00      1.00         5\n",
      "          71       1.00      0.40      0.57         5\n",
      "          72       1.00      1.00      1.00         5\n",
      "          73       0.83      1.00      0.91         5\n",
      "          74       1.00      1.00      1.00         5\n",
      "          75       1.00      1.00      1.00         5\n",
      "          76       1.00      1.00      1.00         5\n",
      "          77       1.00      1.00      1.00         5\n",
      "          78       1.00      1.00      1.00         5\n",
      "          79       1.00      1.00      1.00         5\n",
      "          80       0.83      1.00      0.91         5\n",
      "          81       1.00      1.00      1.00         5\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       1.00      1.00      1.00         5\n",
      "          84       1.00      1.00      1.00         5\n",
      "          85       1.00      1.00      1.00         5\n",
      "          86       1.00      1.00      1.00         5\n",
      "          87       1.00      1.00      1.00         5\n",
      "          88       1.00      0.60      0.75         5\n",
      "          89       0.71      1.00      0.83         5\n",
      "          90       1.00      1.00      1.00         5\n",
      "          91       1.00      1.00      1.00         5\n",
      "          92       1.00      1.00      1.00         5\n",
      "          93       0.83      1.00      0.91         5\n",
      "          94       1.00      1.00      1.00         5\n",
      "          95       1.00      0.80      0.89         5\n",
      "          96       1.00      1.00      1.00         5\n",
      "          97       1.00      1.00      1.00         5\n",
      "          98       1.00      0.60      0.75         5\n",
      "\n",
      "    accuracy                           0.94       495\n",
      "   macro avg       0.95      0.94      0.94       495\n",
      "weighted avg       0.95      0.94      0.94       495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_report = classification_report(ytest, ypred)\n",
    "print('Classification report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "labelencoder = LabelEncoder()\n",
    "leaf_train_df['species'] = labelencoder.fit_transform(leaf_train_df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(leaf_train_df.iloc[:,2:195], leaf_train_df['species'], \n",
    "                                                train_size=0.5,stratify = leaf_train_df['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=4, shuffle=True)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement: {'n_neighbors': 3} avec une accuracy de 0.776\n",
      "\n",
      "Affichage de l'erreur en fonction du choix de K :\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats de la validation croisée :\n",
      "\taccuracy = 0.772 (+/-0.065) for {'n_neighbors': 2}\n",
      "\taccuracy = 0.776 (+/-0.030) for {'n_neighbors': 3}\n",
      "\taccuracy = 0.741 (+/-0.063) for {'n_neighbors': 4}\n",
      "\taccuracy = 0.729 (+/-0.079) for {'n_neighbors': 5}\n",
      "\taccuracy = 0.711 (+/-0.079) for {'n_neighbors': 6}\n",
      "\taccuracy = 0.697 (+/-0.060) for {'n_neighbors': 7}\n",
      "\taccuracy = 0.655 (+/-0.047) for {'n_neighbors': 8}\n",
      "\taccuracy = 0.624 (+/-0.048) for {'n_neighbors': 9}\n",
      "\taccuracy = 0.602 (+/-0.049) for {'n_neighbors': 10}\n",
      "\taccuracy = 0.584 (+/-0.072) for {'n_neighbors': 11}\n",
      "\taccuracy = 0.572 (+/-0.077) for {'n_neighbors': 12}\n",
      "\taccuracy = 0.552 (+/-0.057) for {'n_neighbors': 13}\n",
      "\taccuracy = 0.529 (+/-0.077) for {'n_neighbors': 14}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "params = {'n_neighbors': np.arange(2,15)} \n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "# Créer un classifieur kNN avec recherche d'hyperparamètre par validation croisée\n",
    "gs_knn = GridSearchCV(knn,   params, cv = skf, scoring='accuracy')\n",
    "# Optimiser ce classifieur sur le jeu d'entraînement\n",
    "gs_knn.fit(xtrain, ytrain)\n",
    "print(\"Meilleur(s) hyperparamètre(s) sur le jeu d'entraînement:\",gs_knn.best_params_, \n",
    "      \"avec une accuracy de\", round(gs_knn.cv_results_['mean_test_score'].max(), 3))\n",
    "print(\"\\nAffichage de l'erreur en fonction du choix de K :\")\n",
    "plt.plot(range(2,15), 1 - gs_knn.cv_results_['mean_test_score'], 'o-')\n",
    "plt.ylabel('Erreur')\n",
    "plt.xlabel('K')\n",
    "plt.show()\n",
    "# Afficher les performances \n",
    "print(\"Résultats de la validation croisée :\")\n",
    "for mean, std, params in zip(gs_knn.cv_results_['mean_test_score'], \n",
    "    gs_knn.cv_results_['std_test_score'], gs_knn.cv_results_['params']):\n",
    "    print(\"\\t%s = %0.3f (+/-%0.03f) for %r\" % ('accuracy', mean, std * 2, params ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8202020202020202\n",
      "-***-\n",
      "Confusion matrix:\n",
      "[[5 0 0 ... 0 0 0]\n",
      " [0 5 0 ... 0 0 0]\n",
      " [0 0 5 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 5 0 0]\n",
      " [0 0 0 ... 0 4 0]\n",
      " [0 0 0 ... 0 0 5]]\n",
      "-***-\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      1.00      0.83         5\n",
      "           1       1.00      1.00      1.00         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       0.71      1.00      0.83         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.71      1.00      0.83         5\n",
      "           7       0.71      1.00      0.83         5\n",
      "           8       0.83      1.00      0.91         5\n",
      "           9       1.00      0.80      0.89         5\n",
      "          10       1.00      0.80      0.89         5\n",
      "          11       0.71      1.00      0.83         5\n",
      "          12       0.71      1.00      0.83         5\n",
      "          13       0.50      0.80      0.62         5\n",
      "          14       1.00      1.00      1.00         5\n",
      "          15       1.00      0.80      0.89         5\n",
      "          16       1.00      0.80      0.89         5\n",
      "          17       0.40      0.40      0.40         5\n",
      "          18       1.00      0.80      0.89         5\n",
      "          19       0.67      0.80      0.73         5\n",
      "          20       1.00      1.00      1.00         5\n",
      "          21       0.62      1.00      0.77         5\n",
      "          22       0.83      1.00      0.91         5\n",
      "          23       1.00      0.80      0.89         5\n",
      "          24       1.00      1.00      1.00         5\n",
      "          25       0.83      1.00      0.91         5\n",
      "          26       0.50      0.40      0.44         5\n",
      "          27       0.56      1.00      0.71         5\n",
      "          28       0.50      0.40      0.44         5\n",
      "          29       0.60      0.60      0.60         5\n",
      "          30       0.67      0.40      0.50         5\n",
      "          31       0.75      0.60      0.67         5\n",
      "          32       1.00      0.60      0.75         5\n",
      "          33       1.00      1.00      1.00         5\n",
      "          34       1.00      0.60      0.75         5\n",
      "          35       1.00      1.00      1.00         5\n",
      "          36       0.67      0.40      0.50         5\n",
      "          37       0.71      1.00      0.83         5\n",
      "          38       1.00      0.60      0.75         5\n",
      "          39       0.83      1.00      0.91         5\n",
      "          40       0.83      1.00      0.91         5\n",
      "          41       0.75      0.60      0.67         5\n",
      "          42       1.00      0.40      0.57         5\n",
      "          43       1.00      1.00      1.00         5\n",
      "          44       0.75      0.60      0.67         5\n",
      "          45       1.00      0.60      0.75         5\n",
      "          46       0.67      0.80      0.73         5\n",
      "          47       1.00      0.60      0.75         5\n",
      "          48       1.00      1.00      1.00         5\n",
      "          49       1.00      1.00      1.00         5\n",
      "          50       0.57      0.80      0.67         5\n",
      "          51       0.62      1.00      0.77         5\n",
      "          52       0.62      1.00      0.77         5\n",
      "          53       0.71      1.00      0.83         5\n",
      "          54       1.00      0.80      0.89         5\n",
      "          55       0.80      0.80      0.80         5\n",
      "          56       0.60      0.60      0.60         5\n",
      "          57       1.00      0.20      0.33         5\n",
      "          58       0.67      0.40      0.50         5\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      0.80      0.89         5\n",
      "          61       1.00      1.00      1.00         5\n",
      "          62       1.00      1.00      1.00         5\n",
      "          63       0.83      1.00      0.91         5\n",
      "          64       0.67      0.40      0.50         5\n",
      "          65       0.83      1.00      0.91         5\n",
      "          66       0.83      1.00      0.91         5\n",
      "          67       0.71      1.00      0.83         5\n",
      "          68       0.60      0.60      0.60         5\n",
      "          69       0.50      0.40      0.44         5\n",
      "          70       1.00      1.00      1.00         5\n",
      "          71       1.00      0.40      0.57         5\n",
      "          72       1.00      0.80      0.89         5\n",
      "          73       0.83      1.00      0.91         5\n",
      "          74       1.00      0.80      0.89         5\n",
      "          75       1.00      0.80      0.89         5\n",
      "          76       0.71      1.00      0.83         5\n",
      "          77       1.00      1.00      1.00         5\n",
      "          78       1.00      0.80      0.89         5\n",
      "          79       1.00      0.80      0.89         5\n",
      "          80       0.71      1.00      0.83         5\n",
      "          81       1.00      0.20      0.33         5\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       0.50      0.60      0.55         5\n",
      "          84       1.00      0.80      0.89         5\n",
      "          85       1.00      0.80      0.89         5\n",
      "          86       1.00      0.80      0.89         5\n",
      "          87       1.00      1.00      1.00         5\n",
      "          88       0.57      0.80      0.67         5\n",
      "          89       0.75      0.60      0.67         5\n",
      "          90       1.00      1.00      1.00         5\n",
      "          91       1.00      1.00      1.00         5\n",
      "          92       1.00      0.80      0.89         5\n",
      "          93       1.00      1.00      1.00         5\n",
      "          94       0.80      0.80      0.80         5\n",
      "          95       1.00      1.00      1.00         5\n",
      "          96       0.62      1.00      0.77         5\n",
      "          97       1.00      0.80      0.89         5\n",
      "          98       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.82       495\n",
      "   macro avg       0.85      0.82      0.81       495\n",
      "weighted avg       0.85      0.82      0.81       495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred_knn = gs_knn.predict(xtest)\n",
    "acc_score = accuracy_score(ytest, ypred_knn)\n",
    "conf_matrix = confusion_matrix(ytest, ypred_knn)\n",
    "class_report = classification_report(ytest, ypred_knn)\n",
    "print('Accuracy:', acc_score)\n",
    "print('-***-')\n",
    "print('Confusion matrix:')\n",
    "print(conf_matrix)\n",
    "print('-***-')\n",
    "print('Classification report:')\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "11182020-leaf-classification.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "701d4f28dcd0866722f5108a3a4cbcd08882d26398b6d44117e791d7b8102912"
  },
  "kernelspec": {
   "display_name": "venv_ift603",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
